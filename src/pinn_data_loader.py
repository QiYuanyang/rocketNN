"""
Data loader for Soft-PINN trajectory dataset.

This module provides utilities to load and batch trajectory data
generated by generate_pinn_data.py for training the Soft-PINN model.
"""

import json
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path


class TrajectoryDataset(Dataset):
    """
    PyTorch Dataset for rocket trajectory time-series data.
    
    Each sample contains:
        - params: Input parameters (10D vector with physics features)
        - time: Time points (1D array)
        - position: Position trajectory (N×3 array)
        - velocity: Velocity trajectory (N×3 array)
        - acceleration: Acceleration trajectory (N×3 array)
    """
    
    def __init__(self, data_dir='data/pinn_trajectories', normalize=True):
        """
        Initialize dataset.
        
        Args:
            data_dir: Directory containing trajectory JSON files
            normalize: Whether to normalize data (recommended)
        """
        self.data_dir = Path(data_dir)
        
        # Load metadata and index
        with open(self.data_dir / 'metadata.json', 'r') as f:
            self.metadata = json.load(f)
        
        with open(self.data_dir / 'index.json', 'r') as f:
            self.index = json.load(f)
        
        self.num_trajectories = self.index['num_trajectories']
        self.file_list = self.index['files']
        
        print(f"Loaded dataset: {self.num_trajectories} trajectories")
        
        # Compute normalization statistics if needed
        self.normalize = normalize
        if normalize:
            self._compute_normalization_stats()
    
    def __len__(self):
        return self.num_trajectories
    
    def __getitem__(self, idx):
        """Load a single trajectory."""
        # Load JSON file
        file_path = self.data_dir / self.file_list[idx]
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        # Extract parameters (10D)
        params = np.array([
            data['params']['payload_mass'],
            data['params']['motor_impulse_scale'],
            data['params']['launch_inclination'],
            data['params']['launch_heading'],
            data['params']['surface_wind_speed'],
            data['params']['wind_direction'],
            data['params']['shear_exponent'],
            data['params']['wind_u'],
            data['params']['wind_v'],
            data['params']['drift_potential'],
        ], dtype=np.float32)
        
        # Extract time series
        time = np.array(data['time'], dtype=np.float32)
        
        position = np.stack([
            np.array(data['position']['x'], dtype=np.float32),
            np.array(data['position']['y'], dtype=np.float32),
            np.array(data['position']['z'], dtype=np.float32),
        ], axis=1)  # Shape: (N, 3)
        
        velocity = np.stack([
            np.array(data['velocity']['vx'], dtype=np.float32),
            np.array(data['velocity']['vy'], dtype=np.float32),
            np.array(data['velocity']['vz'], dtype=np.float32),
        ], axis=1)  # Shape: (N, 3)
        
        acceleration = np.stack([
            np.array(data['acceleration']['ax'], dtype=np.float32),
            np.array(data['acceleration']['ay'], dtype=np.float32),
            np.array(data['acceleration']['az'], dtype=np.float32),
        ], axis=1)  # Shape: (N, 3)
        
        # Normalize if enabled
        if self.normalize:
            params = (params - self.param_mean) / (self.param_std + 1e-8)
            time = (time - self.time_mean) / (self.time_std + 1e-8)
            position = (position - self.pos_mean) / (self.pos_std + 1e-8)
            velocity = (velocity - self.vel_mean) / (self.vel_std + 1e-8)
            acceleration = (acceleration - self.acc_mean) / (self.acc_std + 1e-8)
        
        return {
            'params': torch.from_numpy(params),
            'time': torch.from_numpy(time),
            'position': torch.from_numpy(position),
            'velocity': torch.from_numpy(velocity),
            'acceleration': torch.from_numpy(acceleration),
        }
    
    def _compute_normalization_stats(self, sample_size=None):
        """
        Compute mean and std for normalization.
        Uses a subset of data for efficiency if sample_size is specified.
        """
        print("Computing normalization statistics...")
        
        if sample_size is None:
            sample_size = min(1000, self.num_trajectories)
        
        # Sample random trajectories
        indices = np.random.choice(self.num_trajectories, size=sample_size, replace=False)
        
        # Accumulate statistics
        param_list = []
        time_list = []
        pos_list = []
        vel_list = []
        acc_list = []
        
        for idx in indices:
            file_path = self.data_dir / self.file_list[idx]
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            params = np.array([
                data['params']['payload_mass'],
                data['params']['motor_impulse_scale'],
                data['params']['launch_inclination'],
                data['params']['launch_heading'],
                data['params']['surface_wind_speed'],
                data['params']['wind_direction'],
                data['params']['shear_exponent'],
                data['params']['wind_u'],
                data['params']['wind_v'],
                data['params']['drift_potential'],
            ])
            param_list.append(params)
            
            time_list.extend(data['time'])
            
            pos_list.extend([
                [data['position']['x'][i], data['position']['y'][i], data['position']['z'][i]]
                for i in range(len(data['time']))
            ])
            
            vel_list.extend([
                [data['velocity']['vx'][i], data['velocity']['vy'][i], data['velocity']['vz'][i]]
                for i in range(len(data['time']))
            ])
            
            acc_list.extend([
                [data['acceleration']['ax'][i], data['acceleration']['ay'][i], data['acceleration']['az'][i]]
                for i in range(len(data['time']))
            ])
        
        # Compute statistics
        self.param_mean = np.mean(param_list, axis=0).astype(np.float32)
        self.param_std = np.std(param_list, axis=0).astype(np.float32)
        
        self.time_mean = np.mean(time_list).astype(np.float32)
        self.time_std = np.std(time_list).astype(np.float32)
        
        self.pos_mean = np.mean(pos_list, axis=0).astype(np.float32)
        self.pos_std = np.std(pos_list, axis=0).astype(np.float32)
        
        self.vel_mean = np.mean(vel_list, axis=0).astype(np.float32)
        self.vel_std = np.std(vel_list, axis=0).astype(np.float32)
        
        self.acc_mean = np.mean(acc_list, axis=0).astype(np.float32)
        self.acc_std = np.std(acc_list, axis=0).astype(np.float32)
        
        # Save normalization stats
        norm_stats = {
            'param_mean': self.param_mean.tolist(),
            'param_std': self.param_std.tolist(),
            'time_mean': float(self.time_mean),
            'time_std': float(self.time_std),
            'pos_mean': self.pos_mean.tolist(),
            'pos_std': self.pos_std.tolist(),
            'vel_mean': self.vel_mean.tolist(),
            'vel_std': self.vel_std.tolist(),
            'acc_mean': self.acc_mean.tolist(),
            'acc_std': self.acc_std.tolist(),
        }
        
        with open(self.data_dir / 'normalization_stats.json', 'w') as f:
            json.dump(norm_stats, f, indent=2)
        
        print("Normalization statistics saved to normalization_stats.json")
    
    def get_normalization_stats(self):
        """Return normalization statistics for inverse transform."""
        if not self.normalize:
            raise ValueError("Dataset was initialized with normalize=False")
        
        return {
            'param_mean': self.param_mean,
            'param_std': self.param_std,
            'time_mean': self.time_mean,
            'time_std': self.time_std,
            'pos_mean': self.pos_mean,
            'pos_std': self.pos_std,
            'vel_mean': self.vel_mean,
            'vel_std': self.vel_std,
            'acc_mean': self.acc_mean,
            'acc_std': self.acc_std,
        }


def collate_trajectories(batch):
    """
    Custom collate function for batching trajectories.
    
    Since all trajectories have the same length (100 points),
    we can simply stack them.
    """
    params = torch.stack([item['params'] for item in batch])
    time = torch.stack([item['time'] for item in batch])
    position = torch.stack([item['position'] for item in batch])
    velocity = torch.stack([item['velocity'] for item in batch])
    acceleration = torch.stack([item['acceleration'] for item in batch])
    
    return {
        'params': params,        # Shape: (batch_size, 10)
        'time': time,            # Shape: (batch_size, n_points)
        'position': position,    # Shape: (batch_size, n_points, 3)
        'velocity': velocity,    # Shape: (batch_size, n_points, 3)
        'acceleration': acceleration,  # Shape: (batch_size, n_points, 3)
    }


def create_data_loaders(data_dir='data/pinn_trajectories', 
                       batch_size=32, 
                       train_split=0.7, 
                       val_split=0.15,
                       normalize=True,
                       num_workers=4):
    """
    Create train, validation, and test data loaders.
    
    Args:
        data_dir: Directory containing trajectory data
        batch_size: Batch size for data loaders
        train_split: Fraction of data for training (default: 0.7)
        val_split: Fraction of data for validation (default: 0.15)
        normalize: Whether to normalize data
        num_workers: Number of worker processes for data loading
    
    Returns:
        train_loader, val_loader, test_loader, dataset
    """
    # Load full dataset
    dataset = TrajectoryDataset(data_dir=data_dir, normalize=normalize)
    
    # Split indices
    n_total = len(dataset)
    indices = np.arange(n_total)
    np.random.shuffle(indices)
    
    n_train = int(train_split * n_total)
    n_val = int(val_split * n_total)
    
    train_indices = indices[:n_train]
    val_indices = indices[n_train:n_train + n_val]
    test_indices = indices[n_train + n_val:]
    
    print(f"Dataset split: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}")
    
    # Create subset datasets
    from torch.utils.data import Subset
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_trajectories,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_trajectories,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_trajectories,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    return train_loader, val_loader, test_loader, dataset


# Example usage
if __name__ == "__main__":
    # Test the data loader
    print("Testing TrajectoryDataset...")
    
    dataset = TrajectoryDataset(data_dir='data/pinn_trajectories', normalize=True)
    
    # Load a sample
    sample = dataset[0]
    print(f"\nSample structure:")
    print(f"  params shape: {sample['params'].shape}")
    print(f"  time shape: {sample['time'].shape}")
    print(f"  position shape: {sample['position'].shape}")
    print(f"  velocity shape: {sample['velocity'].shape}")
    print(f"  acceleration shape: {sample['acceleration'].shape}")
    
    # Test data loader
    print("\nTesting DataLoader...")
    train_loader, val_loader, test_loader, _ = create_data_loaders(
        batch_size=8,
        train_split=0.7,
        val_split=0.15
    )
    
    batch = next(iter(train_loader))
    print(f"\nBatch structure:")
    print(f"  params: {batch['params'].shape}")
    print(f"  time: {batch['time'].shape}")
    print(f"  position: {batch['position'].shape}")
    print(f"  velocity: {batch['velocity'].shape}")
    print(f"  acceleration: {batch['acceleration'].shape}")
    
    print("\n✓ Data loader test successful!")
